# ğŸ¤– OpenClaw Multi-Agent Container (RunPod Edition)

> **O Solucionador AutÃ´nomo Definitivo para GPUs Cloud**

Este projeto Ã© um container **all-in-one** projetado para implantar enxames de agentes **OpenClaw** rodando 100% locais via **Ollama**, otimizado especificamente para a infraestrutura da **RunPod Community Cloud**.

### ğŸ¯ Objetivo do Projeto
Permitir que desenvolvedores e pesquisadores rodem **mÃºltiplos agentes autÃ´nomos simultÃ¢neos** (atÃ© 10+) em uma Ãºnica GPU potente (como RTX 3090/4090 ou A6000), compartilhando recursos de forma inteligente e eficiente.

### âœ¨ Principais Recursos
- **ğŸ§  MÃºltiplos CÃ©rebros, Uma GPU:** Roda N instÃ¢ncias do OpenClaw isoladas, compartilhando o mesmo backend Ollama.
- **âš¡ Otimizado para RunPod:** ConfiguraÃ§Ã£o zero-touch com reconhecimento automÃ¡tico de IP e portas.
- **ğŸ“¦ Stack Completa:** Inclui OpenClaw (Frontend/Backend) + Ollama Server + Modelos (GLM-4 / Qwen) prÃ©-configurados.
- **ğŸ’¾ PersistÃªncia Inteligente:** Cache de modelos e memÃ³rias de longo prazo sobrevivem a reinicializaÃ§Ãµes do Pod.
- **ğŸ”§ ConfigurÃ¡vel via ENV:** Controle total sobre comportamento do agente, parÃ¢metros de modelo e consumo de VRAM sem tocar em arquivos de config.

---

## ğŸš€ Quick Start

```bash
# RunPod - Use a imagem direto
blacktech/openclaw-multiagent:latest

# VariÃ¡vel OBRIGATÃ“RIA
OPENCLAW_WEB_PASSWORD=sua_senha_forte

# Portas HTTP OBRIGATÃ“RIAS (expor no RunPod)
# 18790 - Agente 1 (sempre)
# 18791 - Agente 2 (se NUM_AGENTS >= 2)
# 18792 - Agente 3 (se NUM_AGENTS >= 3)
# ... atÃ© 18799 (mÃ¡x 10 agentes)
```

**Porta padrÃ£o:** `18790` (primeiro agente)

### ğŸ”“ Como Acessar (Primeiro Acesso)

ApÃ³s o container iniciar (pode levar ~30s para carregar os modelos):

1. VÃ¡ no painel do RunPod e clique em **Connect** > **Expose Port (18790)** ou copie a URL do Proxy.
2. A URL serÃ¡ algo como: `https://{POD_ID}-18790.proxy.runpod.net/overview`
3. VocÃª verÃ¡ uma tela de login.
4. **Input:** `Password (not stored)`
5. **Senha:** Use a mesma definida na ENV `OPENCLAW_WEB_PASSWORD`.

> ğŸ’¡ **Dica:** O OpenClaw usa essa senha apenas para autenticar a sessÃ£o local no browser.

---

## ğŸ“‹ Ãndice

1. [VariÃ¡veis de Ambiente](#-variÃ¡veis-de-ambiente)
   - [ConfiguraÃ§Ã£o de Agentes](#configuraÃ§Ã£o-de-agentes)
   - [ParÃ¢metros do Modelo](#parÃ¢metros-do-modelo-request-api)
   - [Thinking/Behavior](#thinkingbehavior-do-agente)
   - [Contexto e MemÃ³ria](#contexto-e-memÃ³ria)
   - [Ollama Server](#ollama-server-gpu--performance)
2. [Consumo de VRAM](#-consumo-de-vram-glm-47-flash)
3. [Presets por GPU](#-presets-prontos-por-gpu)
4. [Portas e Acesso](#-portas-e-acesso)
5. [PersistÃªncia](#-persistÃªncia-de-dados)
6. [SeguranÃ§a](#-seguranÃ§a)
7. [Troubleshooting](#-troubleshooting)

# =============================================================================
# Arquitetura: Quem Controla O QuÃª?
# =============================================================================

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     OLLAMA SERVER                            â”‚
â”‚  ENV VARS controlam o SERVIDOR:                              â”‚
â”‚  â€¢ OLLAMA_NUM_PARALLEL (requests simultÃ¢neos)                â”‚
â”‚  â€¢ OLLAMA_MAX_LOADED_MODELS (modelos na memÃ³ria)             â”‚
â”‚  â€¢ OLLAMA_KV_CACHE_TYPE (tipo de cache)                      â”‚
â”‚  â€¢ OLLAMA_FLASH_ATTENTION (otimizaÃ§Ã£o GPU)                   â”‚
â”‚  â€¢ OLLAMA_CONTEXT_LENGTH (default se nÃ£o especificado)       â”‚
â”‚  â€¢ OLLAMA_NUM_GPU (layers na GPU)                            â”‚
â”‚  â€¢ OLLAMA_MAX_QUEUE (fila de requests)                       â”‚
â”‚  â€¢ OLLAMA_DEBUG (nÃ­vel de log)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–²
                            â”‚ API Request com params
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OPENCLAW                                â”‚
â”‚  PARAMS sÃ£o enviados na REQUEST:                             â”‚
â”‚  â€¢ temperature (criatividade)                                â”‚
â”‚  â€¢ top_p (nucleus sampling)                                  â”‚
â”‚  â€¢ repeat_penalty (repetiÃ§Ã£o)                                â”‚
â”‚  â€¢ num_ctx (context window desta request)                    â”‚
â”‚  â€¢ think: true/false (modo reasoning)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ VariÃ¡veis de Ambiente

### ConfiguraÃ§Ã£o de Agentes

| VariÃ¡vel | Default | DescriÃ§Ã£o |
|----------|---------|-----------|
| `OPENCLAW_WEB_PASSWORD` | **OBRIGATÃ“RIO** | ğŸ” Senha de acesso aos dashboards |
| `OPENCLAW_NUM_AGENTS` | `3` | NÃºmero de agentes (1-10) |
| `OPENCLAW_AGENT_PREFIX` | `agent` | Prefixo do nome (ex: `agent_1`) |
| `OPENCLAW_BASE_PORT` | `18790` | Porta do primeiro agente |
| `OPENCLAW_MODEL` | `glm-4.7-flash:latest` | Modelo Ollama a usar |
| `OPENCLAW_MODEL_AUTO_PULL` | `true` | Baixar modelo automaticamente |
| `OPENCLAW_WARMUP_ENABLED` | `true` | PrÃ©-carregar modelo na VRAM |

---

### ParÃ¢metros do Modelo (Request API)

> âš ï¸ **Importante:** Esses parÃ¢metros sÃ£o enviados pelo OpenClaw em cada request para o Ollama.

| VariÃ¡vel | Default | DescriÃ§Ã£o |
|----------|---------|-----------|
| `OPENCLAW_TEMPERATURE` | `0.7` | Criatividade (0.0 = determinÃ­stico, 2.0 = muito criativo) |
| `OPENCLAW_TOP_P` | `0.95` | Nucleus sampling (0.0-1.0) |
| `OPENCLAW_REPEAT_PENALTY` | `1.0` | âš ï¸ **CRÃTICO:** Manter em `1.0` para GLM-4.7! |
| `OPENCLAW_NUM_CTX` | `131072` | ğŸ“ Context window (tokens). **Ajuste conforme sua GPU!** |
| `OPENCLAW_MAX_TOKENS` | `32768` | MÃ¡ximo de tokens na resposta |

> ğŸ’¡ **Dica:** `OPENCLAW_NUM_CTX` Ã© o parÃ¢metro mais importante para ajustar conforme sua GPU. Veja a [tabela de VRAM](#-consumo-de-vram-glm-47-flash).

---

### Thinking/Behavior do Agente

| VariÃ¡vel | Default | OpÃ§Ãµes | DescriÃ§Ã£o |
|----------|---------|--------|-----------|
| `OPENCLAW_THINKING_DEFAULT` | `on` | `on` / `off` | ğŸ§  Ativar raciocÃ­nio (reasoning) |
| `OPENCLAW_VERBOSE_DEFAULT` | `off` | `on` / `off` | Modo verbose |
| `OPENCLAW_ELEVATED_DEFAULT` | `on` | `on` / `off` | PermissÃµes elevadas |

---

### Contexto e MemÃ³ria

| VariÃ¡vel | Default | DescriÃ§Ã£o |
|----------|---------|-----------|
| `OPENCLAW_CONTEXT_TOKENS` | `131072` | Limite de tokens do contexto da conversa |
| `OPENCLAW_TIMEOUT_SECONDS` | `600` | Timeout por request (10 min) |
| `OPENCLAW_MAX_CONCURRENT` | `3` | Requests simultÃ¢neos por agente |

---

### Context Pruning (AvanÃ§ado)

> ğŸ§¹ Gerenciamento automÃ¡tico de memÃ³ria quando o contexto fica muito grande.

| VariÃ¡vel | Default | DescriÃ§Ã£o |
|----------|---------|-----------|
| `OPENCLAW_PRUNING_MODE` | `adaptive` | `adaptive` / `aggressive` / `off` |
| `OPENCLAW_KEEP_LAST_ASSISTANTS` | `3` | Quantas respostas recentes manter |
| `OPENCLAW_SOFT_TRIM_RATIO` | `0.3` | Ratio para trim suave (30%) |
| `OPENCLAW_HARD_CLEAR_RATIO` | `0.5` | Ratio para limpeza total (50%) |

---

### Ollama Server - GPU & Performance

> ğŸ–¥ï¸ **Estas variÃ¡veis controlam o servidor Ollama**, nÃ£o os parÃ¢metros da request.

| VariÃ¡vel | Default | DescriÃ§Ã£o |
|----------|---------|-----------|
| `OLLAMA_NUM_PARALLEL` | `4` | Requests paralelos simultÃ¢neos |
| `OLLAMA_KV_CACHE_TYPE` | `q8_0` | Tipo de cache: `q8_0` (economiza ~40% VRAM), `f16` (mÃ¡x qualidade) |
| `OLLAMA_NUM_GPU` | `999` | Layers na GPU (999 = todas) |
| `OLLAMA_KEEP_ALIVE` | `-1` | Tempo modelo na VRAM (-1 = sempre) |
| `OLLAMA_FLASH_ATTENTION` | `1` | Flash Attention (+30-50% velocidade) |
| `OLLAMA_MAX_LOADED_MODELS` | `1` | Modelos simultÃ¢neos na memÃ³ria |
| `OLLAMA_CONTEXT_LENGTH` | `131072` | Context length default do servidor |
| `OLLAMA_MAX_QUEUE` | `512` | Fila de requests antes de rejeitar |

---

### Ollama Server - Rede & Logs

| VariÃ¡vel | Default | DescriÃ§Ã£o |
|----------|---------|-----------|
| `OLLAMA_HOST` | `0.0.0.0` | Interface de rede |
| `OLLAMA_PORT` | `11434` | Porta do servidor Ollama |
| `OLLAMA_DEBUG` | `false` | `false` / `1` (debug) / `2` (trace) |
| `LOG_LEVEL` | `info` | NÃ­vel de log geral |

---

## ğŸ“Š Consumo de VRAM (GLM-4.7-Flash)

> ğŸ“ˆ **O modelo GLM-4.7-Flash** usa ~17GB base + memÃ³ria extra por context length.

| Context Length | VRAM Total | GPU Recomendada |
|----------------|------------|-----------------|
| **4K tokens** | ~17 GB | RTX 3090 |
| **8K tokens** | ~18 GB | RTX 3090/4090 |
| **16K tokens** | ~19 GB | RTX 3090/4090 |
| **32K tokens** | ~20 GB | RTX 3090/4090/A5000 |
| **65K tokens** | ~23 GB | RTX 4090/A5000 |
| **86K tokens** | ~25 GB | RTX 6000 Ada |
| **131K tokens** | ~30 GB | A6000/RTX 6000 Ada |
| **200K tokens** | ~48 GB | A6000/H100/H200 |

> ğŸ’¡ **MLA (Multi-Latent Attention)** do GLM-4.7 economiza ~73% de VRAM no KV cache comparado a modelos tradicionais!

---

## ğŸ¯ Presets Prontos por GPU

### RTX 3090 / RTX 4090 (24GB)

```env
OPENCLAW_NUM_CTX=65536
OPENCLAW_CONTEXT_TOKENS=65536
OLLAMA_CONTEXT_LENGTH=65536
OLLAMA_NUM_PARALLEL=2
```

### RTX A5000 (24GB)

```env
OPENCLAW_NUM_CTX=65536
OPENCLAW_CONTEXT_TOKENS=65536
OLLAMA_CONTEXT_LENGTH=65536
OLLAMA_NUM_PARALLEL=3
```

### RTX A6000 / RTX 6000 Ada (48GB)

```env
OPENCLAW_NUM_CTX=131072
OPENCLAW_CONTEXT_TOKENS=131072
OLLAMA_CONTEXT_LENGTH=131072
OLLAMA_NUM_PARALLEL=4
```

### H100 / H200 (80GB)

```env
OPENCLAW_NUM_CTX=200000
OPENCLAW_CONTEXT_TOKENS=200000
OLLAMA_CONTEXT_LENGTH=200000
OLLAMA_KV_CACHE_TYPE=f16
OLLAMA_NUM_PARALLEL=6
```

---

## ğŸŒ Portas e Acesso

| Porta | ServiÃ§o | Expor PÃºblica? | DescriÃ§Ã£o |
|-------|---------|----------------|-----------|
| `18790` | Agente 1 | **SIM (ObrigatÃ³rio)** | Dashboard Web do Agente 1 |
| `18791` | Agente 2 | **SIM (ObrigatÃ³rio)** | Se `NUM_AGENTS >= 2` |
| `18792` | Agente 3 | **SIM (ObrigatÃ³rio)** | Se `NUM_AGENTS >= 3` |
| `11434` | Ollama API | **NÃƒO (Opcional)** | Use apenas se precisar acessar a API direta (debug). O OpenClaw usa essa porta internamente via `localhost`. |

### Exemplo com 5 Agentes

```env
OPENCLAW_NUM_AGENTS=5
OPENCLAW_WEB_PASSWORD=minha_senha_segura
```

Resultado:
- `agent_1` â†’ porta **18790** (Expor TCP Port)
- `agent_2` â†’ porta **18791** (Expor TCP Port)
- ...
- `agent_5` â†’ porta **18794** (Expor TCP Port)

---

## ğŸ’¾ PersistÃªncia de Dados (Volume RunPod)

Para nÃ£o perder suas memÃ³rias, conversas e modelos baixados ao reiniciar o pod, vocÃª **DEVE** configurar o Volume Path corretamente.

1. No Template do RunPod, configure **Volume Mount Path**: `/workspace`
2. Certifique-se de que seu Container Disk Size Ã© suficiente (mÃ­nimo 50GB recomendado).

**O que Ã© salvo em `/workspace`:**
```
/workspace/
â”œâ”€â”€ .ollama/models/      # Modelos LLM baixados (evita download a cada restart)
â”œâ”€â”€ agents/              # ğŸ§  CÃ‰REBRO DOS AGENTES (MemÃ³rias, sessÃµes, configs)
â”‚   â””â”€â”€ agent_1/
â”‚       â”œâ”€â”€ .openclaw/   # ConfiguraÃ§Ãµes locais do agente
â”‚       â””â”€â”€ workspace/   # Arquivos gerados pelo agente
â”œâ”€â”€ logs/                # HistÃ³rico de logs para debug
â””â”€â”€ .cache/              # Caches de sistema (npm, pip, cuda)
```

> âš ï¸ **AtenÃ§Ã£o:** Se vocÃª nÃ£o montar o volume em `/workspace`, todos os dados serÃ£o perdidos ao desligar o Pod!

### ğŸ’¡ Dica Pro: RunPod Network Volumes (NFS)
Se vocÃª usa a **Secure Cloud**, recomendamos fortemente usar um **Network Volume**.
1. Crie um Network Volume na sua regiÃ£o.
2. Monte-o em `/workspace` no seu template.
3. **BenefÃ­cio:** VocÃª pode destruir o Pod, criar outro (atÃ© com GPU diferente), e **todas as suas memÃ³rias, agentes e histÃ³rico estarÃ£o lÃ¡ intactos**. Ã‰ a forma definitiva de persistÃªncia.

---

## ğŸ”’ SeguranÃ§a e Arquitetura de Isolamento

Para garantir que um agente nÃ£o interfira nas memÃ³rias ou arquivos de outro, implementamos uma arquitetura de isolamento rigorosa:

### 1. ğŸ§  MemÃ³rias e Vector DB Isolados
Cada agente possui seu prÃ³prio banco de dados vetorial (LanceDB/Chroma) localizado em seu diretÃ³rio privado.
- **BenefÃ­cio:** O `Agent 1` (ex: Coder) nunca misturarÃ¡ conhecimento com o `Agent 2` (ex: Writer).
- **Caminho:** `/workspace/agents/agent_{N}/.openclaw/memory`

### 2. ğŸ“‚ Workspaces Separados
Cada agente opera em um diretÃ³rio de trabalho exclusivo (`CWD`).
- **BenefÃ­cio:** Arquivos criados, cÃ³digo gerado e downloads ficam segregados.
- **Estrutura:**
  ```text
  /workspace/agents/
  â”œâ”€â”€ agent_1/ (Porta 18790) ğŸ”’ Privado
  â”œâ”€â”€ agent_2/ (Porta 18791) ğŸ”’ Privado
  â””â”€â”€ ...
  ```

### 3. ğŸ”‘ Tokens de AutenticaÃ§Ã£o Ãšnicos
O sistema gera automaticamente um **Token de ServiÃ§o** Ãºnico para cada agente na inicializaÃ§Ã£o.
- Isso previne que scripts maliciosos em um agente controlem outro agente via API.
- Tokens salvos em: `/workspace/agents/agent_{N}/.openclaw/token`

---

## ğŸ§± SeguranÃ§a

- âœ… **Senha obrigatÃ³ria** - Sem fallback inseguro
- âœ… **Tokens Ãºnicos** - Cada agente tem seu token
- âœ… **Logs mascarados** - Senhas nÃ£o aparecem nos logs
- âœ… **Trusted Proxies** - Configurados para redes internas
- âœ… **CORS automÃ¡tico** - Aceita conexÃµes do RunPod Proxy
- âœ… **Auto-detecÃ§Ã£o RunPod** - Exibe URLs corretas de acesso

---

## ğŸ› Troubleshooting

### Erro: "Out of memory"

Reduza o context:
```env
OPENCLAW_NUM_CTX=65536
OLLAMA_CONTEXT_LENGTH=65536
```

### Erro: "1008 origin not allowed"

JÃ¡ corrigido automaticamente! Se persistir, verifique se estÃ¡ usando a imagem mais recente.

### Modelo nÃ£o carrega

```env
OPENCLAW_MODEL_AUTO_PULL=true
OPENCLAW_WARMUP_ENABLED=true
```

### Performance lenta

```env
OLLAMA_FLASH_ATTENTION=1
OLLAMA_KV_CACHE_TYPE=q8_0
OLLAMA_NUM_PARALLEL=2
```

---

## ğŸ“ Logs

```bash
# Logs de um agente
tail -f /workspace/logs/agent_1.log

# Logs do Ollama
tail -f /workspace/logs/ollama.log

# Via supervisorctl
supervisorctl tail -f openclaw-agent_1
```

---

## ğŸ“œ LicenÃ§a

MIT License - OpenClaw Â© 2024-2026

---

## ğŸ”— Links Ãšteis

- [DocumentaÃ§Ã£o OpenClaw](https://docs.openclaw.ai)
- [DocumentaÃ§Ã£o Ollama](https://ollama.com)
- [RunPod Templates](https://runpod.io/console/templates)
